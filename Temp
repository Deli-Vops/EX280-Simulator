
EX 280 - Steps on exam

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

1/ Users Backup

htpasswd -c -B htpasswd-brake-glass adminuser Developer00

oc create secret generic htpasswd-brake-glass 
  --from-file=htpasswd=htpasswd-brake-glass -n openshift-config

oc edit oauth cluster

  Append to .spec.identityProviders

  - name: brake-glass
    mappingMethod: claim
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpasswd-brake-glass

oc adm policy add-cluster-role-to-user cluster-admin adminuser

oc login -u adminuser -p Developer00

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

2/ Identity Provider Setup

- Create an HTPasswd Identity Provider named htpasswd-provider.
- Store credentials in a secret named htpasswd-secret in openshift-config.
- User Management
  - Add the following users:
    - alice
    - bob
    - charlie
- Group Creation
  - Create a group frontend and add users:
    - alice
    - bob
- Project and RBAC
  - Create a project frontend-dev.
  - Assign:
    - Group frontend: Role edit in frontend-dev.
    - User charlie: Role view in frontend-dev.

htpasswd -c -B -b ex280.htpasswd charlie charlie
htpasswd -B -b ex280.htpasswd alice alice
htpasswd -B -b ex280.htpasswd bob bob

oc create secret generic ex280-htpasswd --from-file=ex280.htpasswd -n openshift-config // La key se crea como ex280-htpasswd en vez de htpasswd

vim ex280-htpasswd.yaml 

apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - name: ex280-htpasswd 
    mappingMethod: claim
    type: HTPasswd
    htpasswd:
      fileData:
        name: ex280-htpasswd

oc apply -f ex280-htpasswd.yaml

oc login -u alice -p alice
oc login -u bob -p bob
oc login -u charlie -p charlie

oc adm groups new frontends alice bob
oc new-project frontend-dev

oc adm policy add-role-to-group edit frontends -n frontend-dev
oc adm policy add-role-to-user view charlie -n frontend-dev

PROJECTS - apollo hades
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

3/ Netpol

- Prepare the env

  oc new-project frontend
  oc new-project backend
  oc new-project test

  # Frontend
  oc run nginx --image=nginx --restart=Always --port=80 -l run=nginx -n frontend
  oc expose pod nginx --port=80 --target-port=80 -n frontend

  oc run busybox --image=busybox --restart=Never -l run=not-nginx -n frontend -- \
    sleep 3600

  # Backend
  oc run httpd --image=httpd --restart=Always --port=80 -l run=httpd -n backend
  oc expose pod httpd --port=80 --target-port=80 -n backend

  # Test
  oc run testnginx --image=nginx --restart=Always --port=80 -l run=test -n test
  oc expose pod testnginx --port=80 --target-port=80 -n test

- Exercise

  Project frontend with:
  - Pod nginx labeled run=nginx
  - Pod busybox labeled run=not-nginx
  Project backend with:
  - Pod httpd labeled run=httpd
  Project test with:
  - Pod testnginx labeled run=test

- Deny all ingress traffic to the backend namespace by default.
- Create a NetworkPolicy in backend that:
  - Applies only to pods with the label run=httpd
  - Allows only TCP traffic on port 80
  - Allows ingress only from pods in frontend with the label run=nginx

--
Validation that its works

oc exec -n test pod/testnginx -- curl -s httpd.backend.svc.cluster.local
<html><body><h1>It works!</h1></body></html>

oc exec -n frontend pod/nginx -- curl -s httpd.backend.svc.cluster.local
<html><body><h1>It works!</h1></body></html>

View and delete old netpol 

oc get netpol -n backend
NAME                           POD-SELECTOR   AGE
allow-from-all-namespaces      <none>         19m
allow-from-ingress-namespace   <none>         19m

oc delete netpol allow-from-all-namespaces -n backend
networkpolicy.networking.k8s.io "allow-from-all-namespaces" deleted

oc delete netpol allow-from-ingress-namespace -n backend
networkpolicy.networking.k8s.io "allow-from-ingress-namespace" deleted

Without netpol, by default all is allowed

oc exec -n frontend pod/nginx -- curl -s httpd.backend.svc.cluster.local
<html><body><h1>It works!</h1></body></html>

oc exec -n test pod/testnginx -- curl -s httpd.backend.svc.cluster.local
<html><body><h1>It works!</h1></body></html>

Create 1st netpoll - Deny all

kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-all
  namespace: backend
  uid: 03a54b83-c8d7-46af-9676-59fcf4d65f60
  resourceVersion: '119860'
  generation: 1
  creationTimestamp: '2025-07-23T10:11:04Z'
  managedFields:
    - manager: Mozilla
      operation: Update
      apiVersion: networking.k8s.io/v1
      time: '2025-07-23T10:11:04Z'
      fieldsType: FieldsV1
      fieldsV1:
        'f:spec':
          'f:policyTypes': {}
spec:
  podSelector: {}
  policyTypes:
    - Ingress
status: {}

Create 2nd netpoll - Allow to app on backend labeled from labeled pod on frontend project

kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-front-frontend-labeled
  namespace: backend
  uid: c4ce245c-3ed6-4454-ae20-ec73b107c1e8
  resourceVersion: '121359'
  generation: 1
  creationTimestamp: '2025-07-23T10:13:31Z'
  managedFields:
    - manager: Mozilla
      operation: Update
      apiVersion: networking.k8s.io/v1
      time: '2025-07-23T10:13:31Z'
      fieldsType: FieldsV1
      fieldsV1:
        'f:spec':
          'f:ingress': {}
          'f:podSelector': {}
          'f:policyTypes': {}
spec:
  podSelector:
    matchLabels:
      run: httpd
  ingress:
    - ports:
        - protocol: TCP
          port: 80
      from:
        - podSelector:
            matchLabels:
              run: nginx
          namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: frontend
  policyTypes:
    - Ingress
status: {}


////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////








////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Authenticación htpasswd
creación de usuarios
eliminación kubeadmin
creación de grupos
roles a usuarios
roles a grupos

Creación de projects

Netpols

Deployment - project grapes
- Creación de deployment desde cero con secret o configmap como variable de entorno
- probes

Escalado manual 

    Your girlfriend isn’t scared of a ghost, but if you scale the casper deployment in the ghosts project from 1 to 10 replicas, even it will be terrified.”

HPA - confucio

    Scale the kungfu-panda deployment in the confucio project with an HPA — watch it grow from 1 to 5 replicas when CPU hits 50%, faster than a panda chasing bamboo! Even Confucius would be impressed

LimitRange y Quotas - napoleon

instalación con helm - videoclub

must-gather

routes / Cliente a route cifrado, de route a pod sin cifrar con una custom cert - duck

      BASE_DOMAIN=$(oc get ingresses.config.openshift.io cluster -o jsonpath='{.spec.domain}')
      HOSTNAME="mini-duck.apps.${BASE_DOMAIN}"
      openssl genrsa -out mini-duck.key 2048
      openssl req -new -key mini-duck.key -out mini-duck.csr -subj "/CN=${HOSTNAME}"
      openssl x509 -req -in mini-duck.csr -signkey mini-duck.key -out mini-duck.crt -days 365 \
        -extfile <(printf "subjectAltName=DNS:${HOSTNAME}")

template de projects - bull

deployment con problemas - wheel

deployment con almacenamiento nfs - Pendiente

Service account con anyuid y deployment que no arranca (apache que debe acceder como root) - comanche

    oc adm policy add-scc-to-user anyuid -z comanche-sa -n ghosts
